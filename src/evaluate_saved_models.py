#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæ¨¡å‹è¯„æµ‹è„šæœ¬
ä¸“é—¨ç”¨äºè¯„æµ‹saved_modelsç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶
"""

import torch
import numpy as np
import logging
import os
import glob
from pathlib import Path

from src.settings import Settings
from src.trainer_adapter import NPZTrainer
from src.model.TrafficForecasting import AutoSTF
from src.DataProcessingAdapter import NPZDataProcessing
from src.model.mode import Mode
from ev import write_result


def setup_logger(log_filename=None):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    import datetime
    import sys

    # åˆ›å»ºlogsç›®å½•
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)

    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    if log_filename is None:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"{log_dir}/evaluation_saved_models_{timestamp}.log"

    # åˆ›å»ºlogger
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # æ¸…é™¤å·²æœ‰çš„handlersï¼ˆé¿å…é‡å¤ï¼‰
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # åˆ›å»ºformatter
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")

    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_filename, mode="w", encoding="utf-8")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    # è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
    logger.info(f"æ—¥å¿—æ–‡ä»¶ä¿å­˜åˆ°: {log_filename}")
    logger.info("=" * 80)

    return log_filename


def parse_model_name(model_path):
    """ä»æ¨¡å‹æ–‡ä»¶åè§£ææ•°æ®é›†ä¿¡æ¯"""
    model_name = Path(model_path).stem
    logging.info(f"è§£ææ¨¡å‹åç§°: {model_name}")

    # è§£ææ¨¡å‹åç§°æ ¼å¼: {dataset_prefix}_{variant}_AutoSTF_best
    parts = model_name.split("_")

    if len(parts) >= 2:
        prefix = parts[0]  # bjs, guomao, xyl
        variant = parts[1]  # 0, 8

        # æ„é€ å®Œæ•´çš„æ•°æ®é›†åç§°
        dataset = f"{prefix}_True_True_{variant}_small"
        settings = dataset

        logging.info(f"æ¨æ–­æ•°æ®é›†: {dataset}")
        return dataset, settings
    else:
        logging.error(f"æ— æ³•è§£ææ¨¡å‹åç§°: {model_name}")
        return None, None


def evaluate_single_model(model_path, device="cuda:0", save_dir="./evaluation_results"):
    """è¯„æµ‹å•ä¸ªæ¨¡å‹"""
    logging.info(f"å¼€å§‹è¯„æµ‹æ¨¡å‹: {model_path}")

    # è§£ææ•°æ®é›†ä¿¡æ¯
    dataset, settings_file = parse_model_name(model_path)
    if not dataset or not settings_file:
        logging.error(f"æ— æ³•è§£ææ¨¡å‹ {model_path}")
        return None

    # æ£€æŸ¥è®¾ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    settings_path = f"model_settings/{settings_file}.yaml"
    if not os.path.exists(settings_path):
        logging.error(f"è®¾ç½®æ–‡ä»¶ä¸å­˜åœ¨: {settings_path}")
        return None

    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    # data_path = f"data/{dataset}"
    # if not os.path.exists(data_path):
    #     logging.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
    #     return None

    try:
        # åŠ è½½è®¾ç½®
        settings = Settings()
        settings.load_settings(settings_file)

        # æ•°æ®å¤„ç†
        logging.info("åŠ è½½æ•°æ®é›†...")
        NPZdata = NPZDataProcessing(
            dataset=dataset,
            train_prop=settings.data.train_prop,
            valid_prop=settings.data.valid_prop,
            num_sensors=settings.data.num_sensors,
            in_length=settings.data.in_length,
            out_length=settings.data.out_length,
            in_channels=3,
            batch_size_per_gpu=settings.data.batch_size,
        )

        scaler = NPZdata.scaler
        dataloader = NPZdata.dataloader
        adj_mx_gwn = [torch.tensor(i).to(device) for i in NPZdata.adj_mx_gwn]
        adj_mx = [torch.tensor(NPZdata.adj_mx_dcrnn).to(device), adj_mx_gwn, torch.tensor(NPZdata.adj_mx_01).to(device)]

        # è®¾ç½®maskæ”¯æŒçš„é‚»æ¥çŸ©é˜µ
        mask_support_adj = [torch.tensor(i).to(device) for i in NPZdata.adj_mx_01]

        # è®¡ç®—scaleåˆ—è¡¨
        scale_list = []
        for i in range(3):
            scale_list.append(int(settings.data.in_length / 3))

        # åˆ›å»ºæ¨¡å‹é…ç½®
        class Config:
            def __init__(self):
                self.scale_list = scale_list
                self.num_sensors = settings.data.num_sensors
                self.in_length = settings.data.in_length
                self.hidden_channels = settings.model.hidden_channels
                self.num_mlp_layers = settings.model.num_mlp_layers
                self.scale_num = settings.model.scale_num
                self.IsUseLinear = settings.model.IsUseLinear
                self.num_linear_layers = settings.model.num_linear_layers
                self.layer_names = settings.model.layer_names
                self.num_temporal_search_node = settings.model.num_temporal_search_node
                self.temporal_operations = settings.model.temporal_operations
                self.num_spatial_search_node = settings.model.num_spatial_search_node
                self.spatial_operations = settings.model.spatial_operations
                self.num_att_layers = settings.model.num_att_layers
                self.num_hop = settings.model.num_hop

        config = Config()

        # åˆå§‹åŒ–æ¨¡å‹
        model = AutoSTF(
            in_length=settings.data.in_length,
            out_length=settings.data.out_length,
            mask_support_adj=mask_support_adj,
            adj_mx=adj_mx,
            num_sensors=settings.data.num_sensors,
            in_channels=3,
            out_channels=settings.data.out_channels,
            hidden_channels=settings.model.hidden_channels,
            end_channels=settings.model.end_channels,
            layer_names=settings.model.layer_names,
            config=config,
            device=device,
        )

        # åŠ è½½æ¨¡å‹æƒé‡
        logging.info(f"åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        checkpoint = torch.load(model_path, map_location=device)

        if "net" in checkpoint:
            model.load_state_dict(checkpoint["net"])
            logging.info("æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ (net)")
        else:
            model.load_state_dict(checkpoint)
            logging.info("æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ (direct)")

        # åœ¨æµ‹è¯•é›†ä¸Šè¯„æµ‹
        logging.info("åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œè¯„æµ‹...")
        model.eval()

        all_predictions = []
        all_truths = []

        test_loader = dataloader["test_loader"].get_iterator()

        with torch.no_grad():
            for batch_idx, (x, y) in enumerate(test_loader):
                x = torch.tensor(x, dtype=torch.float32).to(device)
                y = torch.tensor(y, dtype=torch.float32).to(device)

                # ä½¿ç”¨ONE_PATH_FIXEDæ¨¡å¼è¿›è¡Œé¢„æµ‹
                model.set_mode(Mode.ONE_PATH_FIXED)
                pred = model(x)

                # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœï¼ŒçœŸå®æ ‡ç­¾ä¸éœ€è¦åæ ‡å‡†åŒ–
                pred_denorm = scaler.inverse_transform(pred.cpu().numpy())
                truth_original = y.cpu().numpy()  # çœŸå®æ ‡ç­¾æœ¬èº«å°±æ˜¯åŸå§‹æ•°æ®

                all_predictions.append(pred_denorm)
                all_truths.append(truth_original)

                if batch_idx % 10 == 0:
                    logging.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx}")

        # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
        predictions = np.concatenate(all_predictions, axis=0)
        truths = np.concatenate(all_truths, axis=0)

        logging.info(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions.shape}")
        logging.info(f"çœŸå®æ ‡ç­¾å½¢çŠ¶: {truths.shape}")

        # ä¿å­˜é¢„æµ‹ç»“æœ
        os.makedirs(save_dir, exist_ok=True)

        model_name = Path(model_path).stem
        save_path = os.path.join(save_dir, f"{dataset}_{model_name}_results.npz")

        np.savez(save_path, prediction=predictions, truth=truths, dataset=dataset, model_path=model_path)

        logging.info(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

        # è®¡ç®—è¯„æµ‹æŒ‡æ ‡
        logging.info("è®¡ç®—è¯„æµ‹æŒ‡æ ‡...")
        write_result(save_path)

        csv_path = save_path.replace(".npz", ".csv")
        if os.path.exists(csv_path):
            logging.info(f"è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {csv_path}")
            with open(csv_path, "r") as f:
                content = f.read()
                logging.info("è¯„æµ‹æŒ‡æ ‡:")
                logging.info(content)

        return save_path

    except Exception as e:
        logging.error(f"è¯„æµ‹æ¨¡å‹ {model_path} æ—¶å‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        return None


def main():
    log_filename = setup_logger()

    logging.info("ğŸš€ å¼€å§‹æ‰¹é‡è¯„æµ‹saved_modelsä¸­çš„æ¨¡å‹")

    # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
    model_files = glob.glob("saved_models/*.pth")

    if not model_files:
        logging.error("åœ¨saved_models/ç›®å½•ä¸­æœªæ‰¾åˆ°.pthæ–‡ä»¶")
        return

    logging.info(f"æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    save_dir = "./evaluation_results"
    os.makedirs(save_dir, exist_ok=True)

    success_count = 0

    for i, model_path in enumerate(model_files, 1):
        logging.info(f"\n[{i}/{len(model_files)}] è¯„æµ‹æ¨¡å‹: {os.path.basename(model_path)}")

        result = evaluate_single_model(model_path, device, save_dir)
        if result:
            success_count += 1
            logging.info(f"âœ… æ¨¡å‹ {os.path.basename(model_path)} è¯„æµ‹æˆåŠŸ")
        else:
            logging.error(f"âŒ æ¨¡å‹ {os.path.basename(model_path)} è¯„æµ‹å¤±è´¥")

    logging.info(f"\nğŸ‰ æ‰¹é‡è¯„æµ‹å®Œæˆ!")
    logging.info(f"æˆåŠŸè¯„æµ‹: {success_count}/{len(model_files)} ä¸ªæ¨¡å‹")
    logging.info(f"è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_filename}")

    # æ˜¾ç¤ºæ‰€æœ‰CSVç»“æœ
    csv_files = glob.glob(os.path.join(save_dir, "*.csv"))
    if csv_files:
        logging.info(f"\nğŸ“Š è¯„æµ‹ç»“æœæ±‡æ€»:")
        for csv_file in csv_files:
            logging.info(f"\n{os.path.basename(csv_file)}:")
            with open(csv_file, "r") as f:
                content = f.read()
                logging.info(content)


if __name__ == "__main__":
    main()
